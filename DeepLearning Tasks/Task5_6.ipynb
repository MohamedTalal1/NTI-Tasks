{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3498388c",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adb0fe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import Conv2D, MaxPooling2D, MaxPool2D, Flatten, Dense, Input, Reshape, Dropout, InputLayer\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from keras.models import Sequential, Model\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MeanSquaredError, SparseCategoricalCrossentropy\n",
    "from keras.metrics import SparseCategoricalAccuracy\n",
    "np.random.seed(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0599695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(os. getcwd() , r\"E:\\vs code\\DeepLearning Task\\train\")\n",
    "test_dir = os.path.join (os.getcwd(), r\"E:\\vs code\\DeepLearning Task\\test\")\n",
    "categories = ['benign','malignant']\n",
    "y_train = []\n",
    "x_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "Batch_Size = 32\n",
    "INIT_LR = 1e-4\n",
    "EPOCHES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52807273",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "  path = os.path.join(train_dir, category)\n",
    "  for img in os.listdir(path):\n",
    "    img_path = os.path.join(path, img)\n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.resize(image, (224,224))\n",
    "    image = image / 255\n",
    "    x_train.append(image)\n",
    "    y_train.append(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9607714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "  path = os.path.join(test_dir, category)\n",
    "  for img in os.listdir(path):\n",
    "    img_path = os.path.join(path, img)\n",
    "    image = cv2.imread(img_path)\n",
    "    image - cv2.resize(image, (224,224))\n",
    "    image = image / 255\n",
    "    x_test.append(image)\n",
    "    y_test.append(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e6faf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "lb = LabelBinarizer ()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_train = to_categorical(y_train,num_classes=2)\n",
    "y_test = lb.fit_transform(y_test)\n",
    "y_test = to_categorical (y_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e76943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train, dtype= 'float32')\n",
    "y_train = np.array(y_train,dtype= 'float32')\n",
    "x_test = np.array(x_test,dtype= 'float32')\n",
    "y_test = np.array(y_test, dtype= 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bb9cbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2637, 224, 224, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "124be393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(660, 224, 224, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e2afee",
   "metadata": {},
   "source": [
    "## 5) Only CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b93f9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential ()\n",
    "model.add(InputLayer (input_shape= (224, 224, 3)))\n",
    "model.add(Conv2D (32, kernel_size= (2,2),activation='relu')) \n",
    "model.add (MaxPooling2D (pool_size=(2,2))) \n",
    "model.add(Conv2D (64, kernel_size= (2,2),activation='relu')) \n",
    "model.add(MaxPooling2D (pool_size= (2,2))) \n",
    "model.add(Conv2D (128,kernel_size= (2,2),activation='relu')) \n",
    "model.add(MaxPooling2D(pool_size= (2,2))) \n",
    "model.add(Flatten ()) \n",
    "\n",
    "model.add(Dense (128, activation='relu')) \n",
    "model.add (Dropout (0.5))\n",
    "model.add (Dense (2,activation= 'softmax' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b07f101",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile (optimizer=Adam(learning_rate=INIT_LR, weight_decay=INIT_LR/Batch_Size),\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71199d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "83/83 [==============================] - 61s 708ms/step - loss: 0.6073 - accuracy: 0.6614\n",
      "Epoch 2/10\n",
      "83/83 [==============================] - 61s 732ms/step - loss: 0.4731 - accuracy: 0.7740\n",
      "Epoch 3/10\n",
      "83/83 [==============================] - 60s 727ms/step - loss: 0.4497 - accuracy: 0.7865\n",
      "Epoch 4/10\n",
      "83/83 [==============================] - 60s 720ms/step - loss: 0.4282 - accuracy: 0.7880\n",
      "Epoch 5/10\n",
      "83/83 [==============================] - 61s 735ms/step - loss: 0.4175 - accuracy: 0.7998\n",
      "Epoch 6/10\n",
      "83/83 [==============================] - 59s 705ms/step - loss: 0.4149 - accuracy: 0.8047\n",
      "Epoch 7/10\n",
      "83/83 [==============================] - 59s 716ms/step - loss: 0.3977 - accuracy: 0.8081\n",
      "Epoch 8/10\n",
      "83/83 [==============================] - 61s 729ms/step - loss: 0.4064 - accuracy: 0.8013\n",
      "Epoch 9/10\n",
      "83/83 [==============================] - 61s 732ms/step - loss: 0.3985 - accuracy: 0.8146\n",
      "Epoch 10/10\n",
      "83/83 [==============================] - 60s 728ms/step - loss: 0.3884 - accuracy: 0.8058\n"
     ]
    }
   ],
   "source": [
    "fittinig = model.fit(x_train, y_train,\n",
    "                    epochs=EPOCHES,\n",
    "                    batch_size=Batch_Size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f250a22d",
   "metadata": {},
   "source": [
    "## 6) CNN and ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf6129f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64097f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "# Input layer\n",
    "input_img = Input(shape=input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8fe42c",
   "metadata": {},
   "source": [
    "### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5c5edfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained ResNet50 model without top layers and set input\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=input_img)\n",
    "\n",
    "# Freeze the weights of the ResNet50 layers (optional)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8223c3",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e006f883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CNN layers on top of ResNet50\n",
    "\n",
    "x = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(base_model.output)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "classification_output = Dense(2, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "956b05f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classification model\n",
    "classifier = Model(inputs=input_img, outputs=classification_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "765f7eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the classification model\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c58e76ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "83/83 [==============================] - 135s 2s/step - loss: 0.7129 - accuracy: 0.5142\n",
      "Epoch 2/10\n",
      "83/83 [==============================] - 130s 2s/step - loss: 0.6904 - accuracy: 0.5427\n",
      "Epoch 3/10\n",
      "83/83 [==============================] - 135s 2s/step - loss: 0.6884 - accuracy: 0.5461\n",
      "Epoch 4/10\n",
      "83/83 [==============================] - 139s 2s/step - loss: 0.6856 - accuracy: 0.5472\n",
      "Epoch 5/10\n",
      "83/83 [==============================] - 138s 2s/step - loss: 0.6854 - accuracy: 0.5487\n",
      "Epoch 6/10\n",
      "83/83 [==============================] - 133s 2s/step - loss: 0.6920 - accuracy: 0.5256\n",
      "Epoch 7/10\n",
      "83/83 [==============================] - 125s 2s/step - loss: 0.6900 - accuracy: 0.5461\n",
      "Epoch 8/10\n",
      "83/83 [==============================] - 125s 2s/step - loss: 0.6894 - accuracy: 0.5461\n",
      "Epoch 9/10\n",
      "83/83 [==============================] - 126s 2s/step - loss: 0.6893 - accuracy: 0.5461\n",
      "Epoch 10/10\n",
      "83/83 [==============================] - 126s 2s/step - loss: 0.6893 - accuracy: 0.5461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x153e22735b0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x_train, y_train,\n",
    "               epochs=10,\n",
    "               batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f170cf9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
